{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshualevy\\Documents\\capitalisntDashboard\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "def most_recent_collection(year: str, month: str, date: str) -> pd.DataFrame:\n",
    "    \"\"\"A function for reading in a .csv which regularly changes name.\n",
    "\n",
    "    :param year: A string. Should be in %Y format (four-character year, eg. '2022')\n",
    "    :param month: A string. Should be in %m format (two-character month, eg. '02')\n",
    "    :param date: A string. Should be in %d format (two character date, eg. '09')\n",
    "    \n",
    "    :return df: A Pandas DataFrame containing episode-location-date level observations.\n",
    "    \"\"\"\n",
    "    cities_path_base = 'us_cities_episode_locations-{yyyy}-{mm}-{dd}.csv'.format(\n",
    "        yyyy = year,\n",
    "        mm = month,\n",
    "        dd = date\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(cities_path_base)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interp_dates(start_date: str, end_date: str) -> list:\n",
    "    \"\"\"A function for generating dates that were not collected by `apiCollect.py`\n",
    "\n",
    "    Given the last date of legitimate collection prior to a period of absence and \n",
    "    the next date of legitimate collection following that absence, this function \n",
    "    returns a list of the dates in between.\n",
    "\n",
    "    :param start_date: A string. Should be in '%Y-%m-%d' format. This should be \n",
    "    the last date of legitimate collection before there was a failed collection(s)\n",
    "    (for whatever reason).\n",
    "    :param end_date: A string. Should be in '%Y-%m-%d' format. This should be the\n",
    "    first date of legitimate collection after there was a failed collection(s)\n",
    "    (for whatever reason).\n",
    "\n",
    "    :return interp_dates_list: A list. This contains a list of '%Y-%m-%d'-formatted\n",
    "    dates. It does NOT include `start_date` or `end_date`.\n",
    "    \"\"\"\n",
    "\n",
    "    start_date_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    delta = end_date_dt - start_date_dt\n",
    "\n",
    "    if delta.days <= 1:\n",
    "        raise Exception(\"Hey, I think your dates don't require interpolation!\")\n",
    "\n",
    "    interp_dates_list = []\n",
    "    for i in range(1, delta.days):\n",
    "        new_day = start_date_dt + timedelta(days=i)\n",
    "        new_day_str = datetime.strftime(new_day, '%Y-%m-%d')\n",
    "        interp_dates_list.append(new_day_str)\n",
    "        \n",
    "\n",
    "\n",
    "    return interp_dates_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_interpolate_df(df: pd.DataFrame, start_interp_date: str, stop_interp_date) -> pd.DataFrame:\n",
    "    \"\"\"Given a DataFrame with missing dates, this function identifies the missing dates and re-sorts the columns to adhere to chronological order (important for interpolation)\n",
    "\n",
    "    :param df: A DataFrame. This should be a DataFrame that is missing some date-columns. Those missing date columns are eventually interpolated.\n",
    "    :param start_interp_date: A string. Should be in '%Y-%m-%d' format. This should be \n",
    "    the last date of legitimate collection before there was a failed collection(s)\n",
    "    (for whatever reason).\n",
    "    :param stop_interp_date: A string. Should be in '%Y-%m-%d' format. This should be the\n",
    "    first date of legitimate collection after there was a failed collection(s)\n",
    "    (for whatever reason).\n",
    "\n",
    "    :return out_df: A DataFrame. This contains all date-columns (including the dates for which collection did not occur), properly sorted for interpolation.\n",
    "    \"\"\"\n",
    "    \n",
    "    dates_to_interp_list = generate_interp_dates(start_interp_date, stop_interp_date)\n",
    "\n",
    "\n",
    "    out_df = df.copy()\n",
    "    out_df = out_df.set_index(['city_name', 'city_id', 'state_id', 'episode_id'])\n",
    "    out_df[dates_to_interp_list] = np.nan\n",
    "\n",
    "    date_cols = list(out_df.columns)\n",
    "\n",
    "    for index, date_str in enumerate(date_cols):\n",
    "        if date_str == start_interp_date:\n",
    "            split_open_index = index + 1\n",
    "            continue\n",
    "        elif date_str == stop_interp_date:\n",
    "            split_close_index = index\n",
    "            break\n",
    "\n",
    "    pre_interp_dates_list = date_cols[0 : split_open_index]\n",
    "    post_interp_dates_list = date_cols[split_close_index : -len(dates_to_interp_list)]\n",
    "\n",
    "    ordered_interp_date_cols = pre_interp_dates_list + dates_to_interp_list + post_interp_dates_list\n",
    "    out_df = out_df[ordered_interp_date_cols]\n",
    "\n",
    "    out_df = out_df.interpolate(\n",
    "        method = 'linear',\n",
    "        axis = 'columns',\n",
    "        limit_direction = 'forward',\n",
    "        limit_area = 'inside'\n",
    "    )\n",
    "\n",
    "    return out_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_df_rebiasing(df: pd.DataFrame, start_interp_date: str, stop_interp_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Uses linear interpolation to fill in missing daily downloads data for missing episode-location observations.\n",
    "\n",
    "    This function actually fills in the data and then re-biases it to account for the logarithmic shape of downloads (generally). The first stage is to use a naive linear interpolation between the two dates that are observed. The second stage is to re-bias in favor of downloads closer to release date (see `linear_interp_biasing` for more.)\n",
    "\n",
    "    :param df: A DataFrame. This should be a DataFrame that is missing some date-columns. Those missing date columns are eventually interpolated.\n",
    "    :param start_interp_date: A string. Should be in '%Y-%m-%d' format. This should be \n",
    "    the last date of legitimate collection before there was a failed collection(s)\n",
    "    (for whatever reason).\n",
    "    :param stop_interp_date: A string. Should be in '%Y-%m-%d' format. This should be the\n",
    "    first date of legitimate collection after there was a failed collection(s)\n",
    "    (for whatever reason).\n",
    "\n",
    "    :return df: A DataFrame that has new data filled in. This should have its index reset and then saved.\n",
    "    \"\"\"\n",
    "\n",
    "    dates_to_rebias_list = generate_interp_dates(start_interp_date, stop_interp_date)\n",
    "\n",
    "    if len(dates_to_rebias_list) == 1:\n",
    "        interest_date = dates_to_rebias_list[0]\n",
    "        df[interest_date] = df[[start_interp_date, interest_date, stop_interp_date]].apply(\n",
    "            lambda obs: linear_interp_biasing(obs)\n",
    "        )\n",
    "    else:\n",
    "        for index, interest_date in enumerate(dates_to_rebias_list):\n",
    "            if index == 0:\n",
    "                df[interest_date] = df[[start_interp_date, interest_date, dates_to_rebias_list[index + 1]]].apply(\n",
    "                    lambda obs: linear_interp_biasing(obs),\n",
    "                    axis = 'columns'\n",
    "                )\n",
    "            elif index == len(dates_to_rebias_list) - 1:\n",
    "                df[interest_date] = df[[dates_to_rebias_list[index - 1], interest_date, stop_interp_date]].apply(\n",
    "                    lambda obs: linear_interp_biasing(obs),\n",
    "                    axis = 'columns'\n",
    "                )\n",
    "            else:\n",
    "                df[interest_date] = df[[dates_to_rebias_list[index - 1], interest_date, dates_to_rebias_list[index + 1]]].apply(\n",
    "                    lambda obs: linear_interp_biasing(obs),\n",
    "                    axis = 'columns'\n",
    "                )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interp_biasing(observation : pd.Series) -> float:\n",
    "    \"\"\"Re-biases download-data to biasing downloads toward release date (to mirror the generally logarithmic shape of daily downloads).\n",
    "\n",
    "    :param observation: A Pandas series object. This Series object should have three elements indexed:\n",
    "    0: the downloads on the day prior to the date of interest (as interpolated and potentially re-biased);\n",
    "    1: the downloads of the date of interest (as interpolated and potentially re-biased);\n",
    "    2: the downloads on the day after the date of (as interpolated and potentially re-biased)\n",
    "\n",
    "    We re-bias because we do not want decimal daily downloads, an artefact that may be generated by the linear interpolation process. In the case that there are decimal downloads observed, we round up to the closest integer of downloads in the *next* day, and then round the next day's downloads down (or kept the same). This has the effect of moving downloads earlier in time. We do this to mimic the generally logarithmic shape of downloads over time and resolve the decimal problem.\n",
    "\n",
    "    Because we process the columns of interest from left to right (from earliest to latest), this rounding/biasing montonically shifts biases downloads earlier in time.\n",
    "\n",
    "    (pre, interest, post)\n",
    "    Eg: (34, 34.5, 35) --> (34, 35, 35)\n",
    "    Eg: (867, 868.5, 870) --> (867, 870, 870)\n",
    "\n",
    "    :return interest: the re-biased downloads for the date of interest.\n",
    "    \"\"\"\n",
    "\n",
    "    pre = observation.iloc[0]\n",
    "    interest = observation.iloc[1]\n",
    "    post = observation.iloc[2]\n",
    "\n",
    "    if pre == interest:\n",
    "        return interest\n",
    "    \n",
    "    if pre > interest:\n",
    "        return pre\n",
    "    elif interest.is_integer():\n",
    "        return interest\n",
    "\n",
    "\n",
    "    if pre < interest:\n",
    "        if not interest.is_integer():\n",
    "            # print((pre, interest, post))\n",
    "            # print('NEW VALUE SHOULD BE: {}'.format(math.ceil(post)))\n",
    "            return math.ceil(post)\n",
    "            \n",
    "    return interest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-76ce87a8c6ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m new_df.to_csv('us_cities_episode_locations-{}-{}-{}-INTERPOLATED.csv'.format(\n\u001b[1;32m---> 13\u001b[1;33m     year, month, date),\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'year' is not defined"
     ]
    }
   ],
   "source": [
    "filename_year = '2022'\n",
    "filename_month = '09'\n",
    "filename_date = '20'\n",
    "\n",
    "missing_df = most_recent_collection(filename_year, filename_month, filename_date)\n",
    "\n",
    "\n",
    "new_df = missing_df.copy()\n",
    "new_df = construct_interpolate_df(new_df, '2022-09-17', '2022-09-20')\n",
    "new_df = interp_df_rebiasing(new_df, '2022-09-17', '2022-09-20')\n",
    "new_df = new_df.reset_index(drop=False)\n",
    "new_df.to_csv('us_cities_episode_locations-{}-{}-{}-INTERPOLATED.csv'.format(\n",
    "    filename_year, filename_month, filename_date),\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "new_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a6fdbea366c7b44d686c4b3e88b365fb58ab8fc9919f1d3dc4ca0435b8b7768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
