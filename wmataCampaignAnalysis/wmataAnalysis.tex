\documentclass[11pt, letterpaper, twoside]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage[margin=1in, include foot]{geometry}
\usepackage{ragged2e}
\usepackage[]{hyperref}
\usepackage{apacite}
\usepackage{setspace}
% \usepackage[labelformat=empty]{caption}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{etoolbox}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{afterpage}
\usepackage{floatrow}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{dcolumn}
\usepackage{pdflscape}
\usepackage{adjustbox}
\usepackage{tablefootnote}
\usepackage{multicol}
% \usepackage{showframe}


\setlength{\parindent}{0pt}
\floatsetup[table]{capposition=top}

\title{\singlespacing\textit{Capitalisn't}: WMATA Advertising Campaign Analysis}


\author{
   Utsav Gandhi
   \and
   Joshua Levy
}

\date{\today}

\begin{document}
\begin{titlepage}   
    \maketitle
    \thispagestyle{empty}
\end{titlepage}



\newpage
\pagenumbering{arabic}

\section{About \textit{Capitalisn't} and this investigation}
\textit{Capitalisn't} is a podcast hosted by Luigi Zingales sand Bethany McLean about ``the ways capitalism is -- or more often isn't -- working in our world today ... and what we can do to fix it.'' Released on a bi-weekly schedule, the podcast's reach has steadily grown since its inception in December 2017.\\

In order to expand the show's reach, in 2021 the Stigler Center (the sponsoring organization of the podcast) previously engaged in two advertising campaign with The Economist Media Group and Vox Media group to raise the awareness of the show. Previous evaluations of those campaigns, however, were limited by the construction of the advertising campaigns and the limited availability of high-resolution data.\\

Since then, in an effort to more rigorously evaluate the potential effects of advertising on audience size, the Stigler Center has run a more narrowly defined ad campaign with the Washington Metropolitan Area Transit Authority (WMATA). The design of this was varied in space and in time so as to generate plausible exogeneity in ``treatment'' (exposure to advertising). Moreover, with advance notice the resolution of data collection is considerably higher than was previously available. In this investigation, we employ, ordinary least squares (OLS), regression-discontinuity, and difference-in-differences (DiD) estimation methodologies in an effort to assess the effectiveness of the WMATA ad campaign.

\subsection{Advertising campaigns}
In the course of advertising the podcast, the Stigler Center purchased two kinds of advertisements that were displayed in WMATA stations throughout the greater Washington, D.C.-Maryland-Virginia (DMV) metropolitan area and in train cars on the WMATA system. ``Digital'' advertisements were present on digital-signage in stations and ``static'' advertisements were posted in train cars. The digital ads were posted between January 19th, and February 15th, 2023. The static ads were posted between January 16th and February 12th 2023. For purposes of parsimony, we generally refer to the WMATA ad as begin in effect between January 16th and February 15th, 2023, the outer bounds of the two advertisement periods.\footnote{First-hand accounts reveal that many of the static ads were still posted in train cars well after the paid-for campaign period ended. This was the case as late as the end of March, suggesting that estimation methods may be biased due to inaccurate definition of the treatment period. This is addressed at greater length in subsequent sections.} For these two postings, the Stigler Center paid \$40,000.

\subsection{Data summary}
This investigation primarily focusses on the effect of advertising on \textit{downloads} for \textit{Capitalisn't}.\footnote{Because mobile internet connectivity has improved considerably since the inception of podcasts, streaming podcasts through third-party services such as YouTube has become an increasingly popular alternative to downloading episodes. We restrict our attention to data made available through the podcast's first-party distribution service, Simplecast, which aggregates streams and downloads across a number of third-party podcast aggregation services. This is not, however, an exhaustive measure of the podcasts's audience.} Simplecast, the first-party distribution service that the Stigler Center uses, provides API endpoints that allow us to query for downloads data at varying degrees of temporal- and entity-resolution. Much of this investigation focusses on episode-daily-level downloads data.\\

Additionally, because of the podcast's bi-weekly release schedule downloads 14- and 28-days following release are of interest. Table \ref{episode-level-summary-stats} presents some cursory summary statistics about cumulative downloads to in these intervals. Two aggregate phenomena are worth explicating. First, note that almost every statistic is lower is lower in the full sample (Panel A) than in the most-recent-20 sample (Panel B) of episodes. This represents a secular growth in the podcast over time whereby the early performance of old podcast episodes (when the podcast was small and hand not yet developed a loyal audience) is considerably poorer than that of recent episodes (which receive many more ``first-day downloads''\footnote{``First-day downloads'' here refers to downloads made while the } due to a steady cohort of regular listeners who have ``subscribed' to the podcast''). This phenomenon is underscored by the performance of the most recent episode. Note that the maximum value of the ``Downloads $t=14$'' statistic is greater than the maximum value of the ``Downloads $t=28$'' statistic. That is, the most recent has already received more downloads in 14 days than the next-best-performing episode did in 28 days (and this episode has not yet been released for 28 days).\\

\input{tables/episode-level-summary-stats.tex}

There are over 150 episodes in the \textit{Capitalisn't} catalog. However, because of the secular growth phenomenon identified above and because of the (im)plausible effect of treatment on episodes ``deep'' in the back-catalog of episodes, much of the analysis is restricted to more recently released episodes --- often to the 50 most recently released episodes, or episodes released since the change of hosts in 2020, for example.

\subsection{Advertising effectiveness}
In short, advertising of the type conducted in the WMATA campaign is ineffective.

\section{Motivating Figures}
Many of the short- and long-run phenomena associated with podcast performance are well summarized by a handful of motivating data visualizations. Consider, for example, Figure \ref{fig:all_1142842_cumul_perf} below. Each of these barbell plots display cumulative downloads 1, 14, 28, and 42 days following release.\footnote{Figure qwer does this for a sample of the 20 most recent recent episodes (and is available in the Appendix), the newest of which may not have been released for sufficiently long for all data points to be available.} Two trends are noteworthy. First, as explicated above, there appears to be a secular growth in the performance of the podcast over time. This is borne out in first-day downloads (supposing that listeners who download the podcast on the day of release are ``die-hard'' listeners who are not particularly swayed by the topic presented in a single episode.)

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/all_1142842_day_cumul_perf.png}
  \caption{}
  \label{fig:all_1142842_cumul_perf}
\end{figure}

Secondly, and perhaps more importantly, this is also clear in the mid-run performance of new episodes relative to old episodes. That is, even if listeners are not of the die-hard type, there are a growing number of listeners who are ``loyal'' (will eventually get around to the episode even if it isn't their highest priority). Though the logic for identifying the loyal listeners is fuzzier than that for identifying die-hard listeners, a number of mid-run phenomena constitute evidence for their growing number. Consider that for the first year of the podcast's run (between March 2018 and June 2019), the ``tail'' of episode downloads was almost constant as symbolized by the almost flat trend line that would run through all of the blue, 42-day downloads. However, following this period the ``growing'' barbell corresponds to greater gaps between first-, 14-, 28-, and 42-day downloads. These growing gaps suggest that listeners either a) continue to discover the podcast for the first time and listen to the back catalog; or b) have already discovered the podcast in the past, and are of the loyal type and will eventually get around to listening a given episode following its release. The widening of the barbell over time represents a growing loyal audience.\\

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/recent_podcast_moving_avg_decomp.png}
  \caption{}
  \label{fig:recent_moving_avg_perf_decomp}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/recent_20_episodes_cumul_perf.png}
  \caption{}
  \label{fig:recent_20_cumul_perf}
\end{figure}

The presence of this ``long-tail'' of downloads is further evidenced by Figures \ref{fig:recent_moving_avg_perf_decomp} and \ref{fig:recent_20_cumul_perf}.\footnote{Full-sample equivalents of these figures are available in the appendix.} In Figure \ref{fig:recent_moving_avg_perf_decomp}, the 14-day leading average --- a statistic selected to try to capture the performance of an episode while it is the most recently released episode for a biweekly release cadence --- of the podcast's cumulative (all episodes) downloads is decomposed into three parts: Most-recent, next five, and older. as expected, on any given day, the most-recently released episode constitutes a majority of downloads. However, the subsequent categories also constitute a non-negligible share of downloads suggesting that people continue to listen to the back-catalog of episodes. Additionally, note that whereas the most-recent episode download performance is relatively volatile due to the performance of \textit{only the current episode}, the performance of the back catalog is smoother.\footnote{This phenomenon is most pronounced during the period in late mid-February through March 2023 because the podcast briefly switched to a weekly release cadence (causing a mechanical increase in the 14-day leading average). The next-five, and older-episode downloads, however, are smoothed over a number of episodes.}\\

Figure \ref{fig:recent_20_cumul_perf} also exhibits the evolving performance of the podcast but at the episode level. Cumulative downloads are plotted on the vertical against days since release on the horizontal. Newer episodes are represented in darker shades of blue. The logarithmic shape of cumulative downloads is to be expected. The ``flattening'' of the curve against the vertical axis, however, represents better first-n-day performance of a given episode and the seemingly higher-valued asymptote that newer episodes' cumulative downloads approach represents a growing audience that consistently listens to the growing back-catalog of episodes.\\

In short, there appears to be a considerable weight of evidence that suggests that \textit{Capitalisn't} is a podcast that is growing over time, independent of paid-for advertising efforts. Thus, against this backdrop of secular growth, paid-for advertising would have to be justified by exceptional returns to expenditure.

\section{Policy Evaluation}
This section will report increasingly well-specified estimates of whether the WMATA advertising campaign had significant effects on the performance of the podcast.

\subsection{Naive Episode-Level OLS}
The most naive specification to test whether the ad campaign has an effect would test for whether cumulative downloads (at some point in time) are higher for episodes that are ``treated'' by the ad campaign than those that are not. An episode-level OLS estimation as decribed above may be specified as follows:

\begin{equation}
  \label{ep-level-naive-ols}
  \text{CumulativeDownloads}_i = \alpha + \beta_1\text{Advertisement}_i + X_i +\varepsilon_i, 
\end{equation}

where for each episode $i$, cumulative downloads would be estimated as a function whether or not an episode has experienced the Advertisement (a binary variable) treatment. Thus, we would expect $\beta_1$, the coefficient of interest, to be positive. In (\ref{ep-level-naive-ols}) $X_i$ is a vector of episode-level control variables that might also be correlated with cumulative downloads. The results of a series of OLS estimates of the above specification are presented in Table \ref{tab:ep-level-naive-ols}. In this table, Columns (1) through (3) present regression estimates where the dependent variable is episode-level cumulative downloads at 14 days following release. Columns (4) through (8) present estimates where the dependent variable is cumulative downloads 28 days following release. These definitions of performance are preferred because of the podcast's usual biweekly release cadence.\\

Though the regression estimates presented in Table \ref{tab:ep-level-naive-ols} do not exhibit statistical significance for either the WMATA or Economist/Vox Ad campaign indicator variables, the ``Trailing Average'' regressors exhibit high levels of statistical significance across specifications.\footnote{These regressors are constructed as the the average cumulative downloads of the previous five ($n=5$) episodes 14 ($t=14$) or 28 days ($t=28$) following release.} Notably, the central point estimates are all less than 1 (Column (8) results are explicated at greater length below). This would suggest that podcast performance as defined by the relevant dependent variable is \textit{decaying} over time. For example, in the terms of the estimates presented in Column (1), an episode's cumulative downloads 14 days following release is approximately 97.4\% of the average of the previous five episodes at the same time following their own releases. This runs counter to the story represented by the summary statistics and motivating figures above: that there appears to be a largely secular pattern of growth in downloads (consider the upward trend of light-blue points in Figure \ref{fig:all_1142842_cumul_perf}). Note however, that he for all of these coefficient estimates, 1 lies on or within the 95\% confidence interval. In fact, the average upper-bound of the confidence intervals for the Trailing Average regressor in these 7 specifications is 1.002. A 2\% growth rate relative to the average of the previous 5 episodes suggests a doubling time of approximately 35 periods. With a biweekly release cadence, this corresponds to approximatley 1 year and 5 months, not entirely inconsistent with the growth that is visually described in Figure \ref{fig:all_1142842_cumul_perf}. Of course, the confidence interval on the central point estimates also includes values considerably below 1. This is consistent with the performance of the podcast not monotonically improving. On balance, I would contend that these estimates are weak evidence of the podcast's growth over time.\\

\begin{landscape}
  \input{tables/ep-level-naive-ols.tex}
\end{landscape}

The ``WMATA Digital Ad'' and ``Economist/Vox Ad'' regressors are both indicator variables that are coded as 1 if the episode was aired during either of these treatment periods. In short, they exhibit no meaningful degree of statistical significance. That is, after controlling for the growth of the podcast over time with the Trailing Average regressor, the advertisement campaigns do not appear to be predictive of higher cumulative downloads for treated episodes, as one might hypothesize. This is further tested in other specifications below. Column (8) in Table \ref{tab:ep-level-naive-ols} is included as another test of the podcast's serial autocorrelation with itself (at the episode-level). Note that both of the Trailing Average regressors exhibit (at varying levels of statistical significance). The 14-day regressor appears to be both very statistically significant and very positive and greater than 1. The coefficient is on the 28-day regressor however, is statistically significant at the 10\% level and negative. Both of these coefficients have very large conffidence intervals but the summation of the two suggest a time-trend that is positive, consistent with the evidence presented above.

\subsection{Regression Kinks}


\subsection{DMV Diff-in-Diff}

% \begin{table}[!htbp] \centering 
%   \caption{}
%   \label{}
% \begin{tabular}{@{\extracolsep{5pt}}lcc}
% \\[-1.8ex]\hline
% \hline \\[-1.8ex]
%  & \multicolumn{2}{c}{\textit{Dependent variable:}} \\
% \cline{2-3} 
% \\[-1.8ex] & \multicolumn{2}{c}{cumulative\_downloads} \\
% \\[-1.8ex] & (1) & (2)\\
% \hline \\[-1.8ex]
%  log\_days\_since\_release & 2,718.561 & 2,718.561$^{***}$ \\ 
%   &  & (257.808) \\
%   & & \\
%  in\_wmata\_general\_ad &  &  \\
%   &  &  \\
%   & & \\
%  log\_days\_since\_release:in\_wmata\_general\_ad &  &  \\
%   &  &  \\
%   & & \\
%  Constant & 8,144.782 & 8,144.782$^{***}$ \\ 
%   &  & (228.348) \\
%   & & \\
% \hline \\[-1.8ex]
% Observations & 5 & 5 \\
% R$^{2}$ & 0.982 & 0.982 \\
% Adjusted R$^{2}$ & 0.976 & 0.976 \\
% Residual Std. Error (df = 3) & 267.810 & 267.810 \\
% F Statistic (df = 1; 3) & 166.468$^{***}$ & 166.468$^{***}$ \\ 
% \hline
% \hline \\[-1.8ex]
% \textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\
% \end{tabular}
% \end{table}


\end{document}